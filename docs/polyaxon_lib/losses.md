<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L42)</span>
## AbsoluteDifferenceConfig

```python
polyaxon_schemas.losses.AbsoluteDifferenceConfig(input_layer=None, output_layer=None, weights=1.0, name=None, collect=True)
```

Adds an Absolute Difference loss to the training procedure.

`weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a `Tensor` of
shape `[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`.

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.


- __Returns__:

	A scalar `Tensor` representing the loss value.

Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: AbsoluteDifference
  # other model properties
```

or

```yaml
model:
  # other model properties
  loss:
	AbsoluteDifference:
	  input_layer: labels
	  output_layer: dense_out
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L42)</span>
## AbsoluteDifferenceConfig

```python
polyaxon_schemas.losses.AbsoluteDifferenceConfig(input_layer=None, output_layer=None, weights=1.0, name=None, collect=True)
```

Adds an Absolute Difference loss to the training procedure.

`weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a `Tensor` of
shape `[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`.

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.


- __Returns__:

	A scalar `Tensor` representing the loss value.

Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: AbsoluteDifference
  # other model properties
```

or

```yaml
model:
  # other model properties
  loss:
	AbsoluteDifference:
	  input_layer: labels
	  output_layer: dense_out
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L105)</span>
## MeanSquaredErrorConfig

```python
polyaxon_schemas.losses.MeanSquaredErrorConfig(input_layer=None, output_layer=None, weights=1.0, name=None, collect=True)
```

Computes Mean Square Loss.

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: MeanSquaredError
  # other model properties
```

or

```yaml
model:
  # other model properties
  loss:
	MeanSquaredError:
	  input_layer: labels
	  output_layer: dense_out
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L165)</span>
## LogLossConfig

```python
polyaxon_schemas.losses.LogLossConfig(input_layer=None, output_layer=None, weights=1.0, epsilon=1e-07, name=None, collect=True)
```

Computes Huber Loss for DQN.

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.

	- __epsilon__: A small value.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: LogLoss
  # other model properties
```

or

```yaml
model:
  # other model properties
  loss:
	LogLoss:
	  input_layer: labels
	  output_layer: dense_out
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L236)</span>
## HuberLossConfig

```python
polyaxon_schemas.losses.HuberLossConfig(input_layer=None, output_layer=None, weights=1.0, clip=0.0, name=None, collect=True)
```

Computes Huber Loss for DQN.

[Wikipedia link](https://en.wikipedia.org/wiki/Huber_loss)
[DeepMind link](https://sites.google.com/a/deepmind.com/dqn/)

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: HuberLoss
  # other model properties
```

or

```yaml
model:
  # other model properties
  loss:
	HuberLoss:
	  clip: 0.2
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L309)</span>
## ClippedDeltaLossConfig

```python
polyaxon_schemas.losses.ClippedDeltaLossConfig(input_layer=None, output_layer=None, weights=1.0, clip_value_min=-1.0, clip_value_max=1.0, name=None, collect=True)
```

Computes clipped delta Loss for DQN.

[Wikipedia link](https://en.wikipedia.org/wiki/Huber_loss)
[DeepMind link](https://sites.google.com/a/deepmind.com/dqn/)

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.

	- __clip_value_min__: default to -1.

	- __clip_value_max__: default to 1.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: ClippedDeltaLoss
  # other model properties
```

or

```yaml
model:
  # other model properties
  loss:
	ClippedDeltaLoss:
	  clip_value_min: -0.8
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L386)</span>
## SoftmaxCrossEntropyConfig

```python
polyaxon_schemas.losses.SoftmaxCrossEntropyConfig(input_layer=None, output_layer=None, weights=1.0, label_smoothing=0.0, name=None, collect=True)
```

Computes Softmax Cross entropy (softmax categorical cross entropy).

Computes softmax cross entropy between y_pred (logits) and
y_true (labels).

Measures the probability error in discrete classification tasks in which
the classes are mutually exclusive (each entry is in exactly one class).
For example, each CIFAR-10 image is labeled with one and only one label:
an image can be a dog or a truck, but not both.

- __**WARNING__:** This op expects unscaled logits, since it performs a `softmax`

on `y_pred` internally for efficiency.  Do not call this op with the
output of `softmax`, as it will produce incorrect results.

`y_pred` and `y_true` must have the same shape `[batch_size, num_classes]`
and the same dtype (either `float32` or `float64`). It is also required
that `y_true` (labels) are binary arrays (For example, class 2 out of a
total of 5 different classes, will be define as [0., 1., 0., 0., 0.])

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.

	- __label_smoothing__: If greater than `0` then smooth the labels.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: SoftmaxCrossEntropy
  # other model properties
```

or

```yaml
model:
  # other model properties
  loss:
	SoftmaxCrossEntropy:
	  label_smoothing: 0.1
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L474)</span>
## SigmoidCrossEntropyConfig

```python
polyaxon_schemas.losses.SigmoidCrossEntropyConfig(input_layer=None, output_layer=None, weights=1.0, label_smoothing=0.0, name=None, collect=True)
```

Computes Sigmoid cross entropy.(binary cross entropy):

Computes sigmoid cross entropy between y_pred (logits) and y_true
(labels).

Measures the probability error in discrete classification tasks in which
each class is independent and not mutually exclusive. For instance,
one could perform multilabel classification where a picture can contain
both an elephant and a dog at the same time.

For brevity, let `x = logits`, `z = targets`.  The logistic loss is

  x - x * z + log(1 + exp(-x))

To ensure stability and avoid overflow, the implementation uses

  max(x, 0) - x * z + log(1 + exp(-abs(x)))

`y_pred` and `y_true` must have the same type and shape.

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.

	- __label_smoothing__: If greater than `0` then smooth the labels.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: SigmoidCrossEntropy
  # other model properties
```

or

```yaml
model:
  # other model properties
  loss:
	SigmoidCrossEntropy:
	  label_smoothing: 0.1
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L561)</span>
## HingeLossConfig

```python
polyaxon_schemas.losses.HingeLossConfig(input_layer=None, output_layer=None, weights=1.0, name=None, collect=True)
```

Hinge Loss.

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: HingeLoss
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L609)</span>
## CosineDistanceConfig

```python
polyaxon_schemas.losses.CosineDistanceConfig(dim, input_layer=None, output_layer=None, weights=1.0, name=None, collect=True)
```

Adds a cosine-distance loss to the training procedure.

Note that the function assumes that `predictions` and `labels` are already unit-normalized.

- __WARNING__: `weights` also supports dimensions of 1, but the broadcasting does

not work as advertised, you'll wind up with weighted sum instead of weighted
mean for any but the last dimension. This will be cleaned up soon, so please
do not rely on the current behavior for anything but the shapes documented for
`weights` below.

- __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: CosineDistance
  # other model properties
```


----

<span style="float:right;">[[source]](https://github.com/polyaxon/polyaxon/blob/master/polyaxon_schemas/losses.py#L722)</span>
## KullbackLeiberDivergenceConfig

```python
polyaxon_schemas.losses.KullbackLeiberDivergenceConfig(dim, input_layer=None, output_layer=None, weights=1.0, name='KullbackLeiberDivergence', collect=True)
```

Adds a Kullback leiber diverenge loss to the training procedure.

 - __Args__:

	- __input_layer__: The input true values layer, defaults to labels.

	- __output_layer__: The output layer generated by the network,

		default to last layer of the network.
		If the network has multiple output, you should specify which layer to use.
	- __weights__: Optional `Tensor` whose rank is either 0, or the same rank as

		`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
		be either `1`, or the same as the corresponding `losses` dimension).
	- __name__: operation name.

	- __collect__: add to losses collection.


- __Returns__:

	A scalar `Tensor` representing the loss value.

- __Raises__:

	- __ValueError__: If `predictions` shape doesn't match `labels` shape, or `weights` is `None`.


Polyaxonfile usage:

```yaml
model:
  # other model properties
  loss: KullbackLeiberDivergence
  # other model properties
```
